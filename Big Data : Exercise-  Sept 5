	1. Explain the difference between the Map and Reduce phases in the MapReduce paradigm.
		One of the three components of Hadoop is Map Reduce. The first component of Hadoop that is, Hadoop Distributed File System (HDFS) is responsible for storing the file. The second component that is, Map Reduce is responsible for processing the file. 
		MapReduce has mainly 2 tasks which are divided phase-wise. In first phase, Map Phase: Focuses on dividing the input data and producing intermediate key-value pairs. In second phase Reduce Phase: Focuses on aggregating the intermediate key-value pairs to produce the final output.
		Both phases are designed to work in a distributed manner, allowing MapReduce to handle large datasets efficiently across multiple nodes in a cluster.
		
		
	2. What are the advantages of using MapReduce in big data processing?
		MapReduce is a powerful paradigm for processing large datasets across distributed systems. MapReduce provides a scalable, fault-tolerant, and cost-effective framework for processing big data. Its simplicity, efficiency, and flexibility make it a popular choice for a wide range of data processing tasks.
			i. Scalability : MapReduce can scale out automatically by adding more distributed nodes to a cluster. It helps in balancing the load across the cluster/machines, making it possible to handle petabytes of data.
			ii. Fault Tolerance : MapReduce frameworks like Hadoop replicate data across multiple nodes. If a node fails, the system can recover using the replicated data. If one fails -the framework reassigns to another nod, to ensure that processing continues seamlessly, even in face of hw or sw failures.
			iii. Simplicity : MapReduce abstracts the complexity of parallel processing and distributed storage. Developers can focus on writing map and reduce functions without needing to manage the underlying infrastructure.
			iv. Efficiency : The MapReduce framework optimizes the execution of tasks, including scheduling and resource allocation, to maximize efficiency.
			v. Cost-Effectiveness : MapReduce frameworks like Hadoop can run on inexpensive, commodity hardware. This lowers the cost of building and maintaining large-scale data processing systems.
			vi. Flexibility : Users can implement custom map and reduce functions to meet specific needs, allowing for flexible and tailored data processing.
			vii. Extensibility : MapReduce integrates well with other big data technologies and tools, such as HDFS (Hadoop Distributed File System), Hive, Pig, and Spark, enhancing its functionality and use cases. And additional features and optimizations, such as support for more complex workflows and machine learning libraries.
			viii. Data Processing Paradigm : MapReduce is well-suited for batch processing tasks, making it ideal for analyzing large volumes of data that do not require real-time processing. The reduce phase is effective for aggregating results from the map phase, enabling complex aggregations and computations over large datasets.
		
	3. Role of the Shuffle and Sort phase in MapReduce?
		Shuffle and Sort phase is a critical intermediate step in the MapReduce paradigm that occurs between the Map and Reduce phases, involves sorting and grouping the data based on keys so that all values associated with the same key are sent to the same reducer
			i. Shuffle : intermediate key-value pairs(data transfer via network) generated by the mappers are transferred to the reducers(reduced node)
			ii. Sort : Once data is received by reducers, it is sorted by Key. And ensures Key-Value pairs with same Key are grouped together and processed in single reducer. All these efficiently aggregate or process associated values with Key.
		This data is often written to disk or local storage on the mapper nodes before being shuffled to the reducers.
		Sorting is crucial for the efficient execution of the reduce phase because it allows reducers to process data for each key in a sequential and organized manner
		E.g.: 
		Consider a word count example:
			• Map Phase Output: Mapper produces key-value pairs like ("word1", 1), ("word2", 1), ("word1", 1).
			• Shuffle and Sort Phase:
				○ Shuffle: All pairs with "word1" are shuffled to one reducer, and pairs with "word2" to another.
				○ Sort: Within each reducer, the pairs are sorted so that all instances of "word1" are grouped together, and all instances of "word2" are grouped together.
			• Reduce Phase Input: Reducer receives sorted groups of key-value pairs and aggregates them, producing the final counts.
		
	4. Drawbacks of Hadoop over Apache Spark
		These drawbacks don't necessarily mean Hadoop is inferior to Spark; rather, each tool has its own strengths and ideal use cases.
		a. Performance - Hadoop MapReduce model processes data in a series, where each stage involves writing intermediate results to disk - which can be a performance issue, especially for iterative algorithms or queries. Spark - performs in-memory processing, which improves speed and performance for many cases.
		b. Complexity : Hadoop MapReduce can me more complex because of iterative algorithm's requiring multiple stages of processing. Spark uses Higher level APIs like data frames and datasets
		c. Latency: Due to reliance on disk- based storage and batch processing - Hadoop MapReduce often has higher latency. Spark in- memory computing allows for faster data processing and reduced latency (suitable for real-time applications/ Spark streaming)
		d. Ease to use: Spark provides more user-friendly API with support for SQL queries, ML, Graph processing, which can be simpler and more intuitive compared to lower-level APIs of Hadoop MapReduce.
		e. Built-in libraries: Spark includes built-in libraries for SQL(Spark SQL), MLLib, GraphX, which can be used directly without needing additional tools or integration. Whereas Hadoop requires additional tools like Apache Hive, Apache Mahout and Apache Giraph for similar functionalities.
		f. Resource Management: Spark can be more flexible in terms of resource management and scheduling. It can run on various cluster managers such as YARN, Mesos, or Kubernetes. Hadoop’s resource management is primarily managed through YARN, which can be less flexible and more complex to configure and manage compared to Spark's options.
		
		
![image](https://github.com/user-attachments/assets/edfa3449-04a4-4cc3-87d2-21942b166b1d)
